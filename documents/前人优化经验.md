2024.8.12.
v0.4
固定起点2终点1, 宝箱数目为[13, 'norm', 13, 'norm'], 训练1e7步, 同样也遇到了训练一段时间后, value直接崩溃的问题.

v0.4.1
讨论后考虑有如下这些改进方向, 逐步进行尝试, 看能否解决问题:

 return过大, 尝试直接 /10 进行缩放
 logprob 应该是由actor给出传到buffer里面, 而非通过learner重新计算
 value 是否应该也是由actor给出
 增大buffer size到进程数*10, 增大batchsize到进程数*3, 增大消耗/生成比production_consume_ratio=3, 预加载比例 preload_ratio=2
 日志中输出clip后的grad_norm查看是否会顶满max_grad_norm
 减小学习率 2.5e-4 -> 5e-5
2024.8.13.
v0.4.2
修复闪现撞墙的错误奖励
env中计算ratio时忘记除以num_envs
减小训练步长 1e7->5e6
修复total_score忘记加步数得分的问题
日志中加入buffer相关参数 miss_buffer
ent_coef: 1e-2 -> 1e-3
修复奖励中delta treasure错误计算了缺失的宝箱
repeat_step_thre = 0.2 -> 0.4
n_treasure = [13,'norm',13,'norm'] -> ['norm', 13, 'norm']
2024.8.14.
v0.4.3
将 num_minibatches 替换为 minibatch_size, 直接指定minibatch大小
update_epoches: 2 -> 1
norm_adv: True -> False
norm_sigma: 2.0 -> 3.0, 修正 action_space: np.arange(13) -> np.arange(14)
修复env中 ratio 计算错误的问题, 能够按比例调整宝箱数目, 修复reward里面周围步数惩罚的错误.
2024.8.15.
v0.4.4
每步都加入 0.2 的惩罚
num_envs: 10 -> 12
num_steps: 128 -> 512
v0.4.4.1
训练到47293步时, 出现环境范围为None的报错, 导致训练停止

# score 来源于: frame_no, obs, score, terminated, truncated, _env_info = self.env.step(self.action)
加入对score的异常处理, 如果为None则反复进行 env.step. 出现该报错的原因可能是 num_steps 调到了 512 ? 重新减小回 128

num_steps: 512 -> 128
num_envs: 12 -> 10 训练到12h时候再次出现score=None的情况 (确认应该和硬盘空间不足有关), 为了能够今天跑完跑榜任务, 特训一个10个宝箱的
2024.8.16.
无法开启到客户端指定的进程数目, 原因也可能和硬盘空间不足有关, 尽可能多开一点, 16进程一般只能开到12进程

num_envs: 10 -> 12
each_step_punish: 0.2 -> 0.1, 只在后半程训练 ratio > 0.5 中加入步数惩罚
2024.8.17.
v1.0
对v0.4.3的4548步模型接着训练:

采用每步 0.1 的惩罚
num_envs: 12 -> 10, num_steps: 128 -> 256
加入随机起点, 在前50%的episode中智能体初始化在随机的起点上, 后50%只能出现在2号节点上.
learning_rate: 5e-5 -> 2.5e-5, ent_coef: 1e-3 -> 1e-4
2024.8.19.
v1.1
完成v1.0训练, 提升挺大, 部分情况下会出现漏宝箱的问题

关闭动态宝箱奖励, 增大宝箱奖励与惩罚 50 -> 100
增大步数惩罚, 0.1 -> 0.2
n_treasure: norm -> uniform
关闭随机起点 random_start_position_ratio = 0.5 -> 0.0
降低学习率 2.5e-5 -> 1e-5
增大 update_epochs: 1 -> 2,
减小 ent_coef: 1e-4 -> 1e-5
没内存了😢, 不能增大 replay_buffer_capacity: 100 -> 200, train_batch_size: 30 -> 60

2024.8.24.
v1.2
接着v1.1-2930训练1e7步, 但是版本更新到了9.2.2又要重新训练提交, 所以先更新了下版本把v1.2的提交上去.

2024.8.25.
v1.3
v1.2的路线基本固化, 例如0宝箱时候必然会多走很远, 因此重新开始训练, 这次直接使用uniform均匀随机宝箱, 并且由于版本更新导致无法再使用off-policy的buffer策略, 也就是production_consume_ratio,

learning_rate: 1e-5 -> 5e-5
ent_coef: 1e-5 -> 1e-3 v9.2.2貌似只能使用定时循环执行训练, 当前2.5s完成一个256的num_steps, 为了更新20个样本后开始训练, 休息时长给到50s
learner_train_sleep_seconds: 50.0 再次尝试增大buff
replay_buffer_capacity: 100 -> 200
train_batch_size: 30 -> 60
2024.8.26.
v1.2beta1
基于v1.2-4664继续训练, 增多训练次数, 由于v1.2只在少数宝箱时产生多余步数问题, 单独训练0,1宝箱3e6步, 沿用v1.3.1的参数配置:

learner_train_sleep_seconds = 50.0 -> 30.0
total_timesteps = int(1e7) -> int(3e6)
n_treasure = "uniform" -> [0, 1]
v1.2beta2
基于v1.2beta1-195继续训练1e7步, 使用

learning_rate: 5e-5 -> 1e-5
ent_coef: 1e-3 -> 1e-5
n_treasure: [0, 1] -> 'uniform' 由于训练一半不训了, 重新减小
replay_buffer_capacity: 200 -> 100
train_batch_size: 60 -> 30
learner_train_sleep_seconds: 30 -> 20 出现问题, 训练5h11min就终止了, 因为step出现None, 而且0个宝箱时候模型重新回到之前绕路的状态.
2024.8.27.
v1.3.1
继续v1.3-164训练, 使用v1.3配置, 训了6h11mins, leaner终止了.

v1.3.2
v1.3.1_940继续训练, 训了6h21min, leaner莫名其妙终止了.

2024.8.28.
v1.3.3
v1.3.2_958继续训练, 训了6h5min, 照样崩溃了

v1.3.4
v1.3.3_913继续训

learning_rate: 5e-5 -> 2.5e-5
ent_coef: 1e-3 -> 1e-4
v1.2.o9.1
由于最终比赛为9宝箱, 接着v1.2_4664特训9个宝箱, 修改如下

learning_rate: 1e-5
ent_coef: 0 训练4h20min到885步
v1.2.o9.2
接着v1.2.o9.1-885继续训练, 训练到859步, 效果越来越差, 终止训练.

2024.8.29.
v1.2.1
接着v1.2-4664的配置继续训练, 将 ent_coef: 1e-5 -> 0, 训练了6h45min, 955步.

v1.2.2
接着v1.2.1-955继续训练